{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933df03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from report import Report, open_single_report_file, open_5fcv_report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f349726a",
   "metadata": {},
   "source": [
    "# Single report resuls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reports location and information\n",
    "# report_content = open_single_report_file(\"../results/report_0.json\")\n",
    "# if report_content is None:\n",
    "#     print(\"No report found\")\n",
    "# #     exit(1)\n",
    "\n",
    "# report = Report(report_content)\n",
    "# report.clean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4931b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct visualization for single report\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "# Extract metric data\n",
    "plot_metric = \"accuracy\"  # Define the metric to plot\n",
    "rounds, metric_values = report.extract_central_eval_metric(plot_metric)\n",
    "\n",
    "# Plot data\n",
    "ax.plot(rounds, metric_values, '-o', linewidth=2, markersize=6)\n",
    "ax.set_title(f\"Performance Metrics - {plot_metric.capitalize()}\", fontsize=16)\n",
    "ax.set_xlabel(\"Rounds\", fontsize=14)\n",
    "ax.set_ylabel(plot_metric.capitalize(), fontsize=14)\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# Print summary\n",
    "print(f\"======= Results Summary =======\")\n",
    "if \"hyperparams\" in report.report:\n",
    "    print(f\"Hyperparameters: {report.report['hyperparams']}\")\n",
    "print(f\"Final {plot_metric}: {metric_values[-1]:.4f}\")\n",
    "print(f\"Maximum {plot_metric}: {np.max(metric_values):.4f}\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fecdf2",
   "metadata": {},
   "source": [
    "# Multipe report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f234e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reports location and information\n",
    "results_dir = \"../results/\"\n",
    "runs = 3\n",
    "all_reports: list[Report] = []\n",
    "\n",
    "for run in range(runs):\n",
    "    path = os.path.join(results_dir, f\"report_{run}.json\")\n",
    "    report = open_single_report_file(path)\n",
    "    \n",
    "    if report is None:\n",
    "        continue\n",
    "    \n",
    "        \n",
    "    all_reports.append(Report(report))\n",
    "all_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c05a5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_metric = \"loss\"  # Define the metric to plot\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "# Extract metric data\n",
    "ax.set_title(f\"Performance Metrics - {plot_metric.capitalize()}\", fontsize=16)\n",
    "ax.set_xlabel(\"Rounds\", fontsize=14)\n",
    "ax.set_ylabel(plot_metric.capitalize(), fontsize=14)\n",
    "ax.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "for run_report in all_reports:\n",
    "    rounds, metric_values = run_report.extract_central_eval_metric(plot_metric)\n",
    "\n",
    "    # Plot data\n",
    "    ax.plot(rounds, metric_values, '-o', linewidth=2, markersize=6, label=f\"Run {run_report.report['run_id']}\")\n",
    "\n",
    "    print(f\"======= Results Summary for Run {run_report.report['run_id']} =======\")\n",
    "    if \"hyperparams\" in run_report.report:\n",
    "        print(f\"Hyperparameters: {run_report.report['hyperparams']}\")\n",
    "    if \"il_config\" in run_report.report:\n",
    "        print(f\"IL Config: {run_report.report['il_config']}\")\n",
    "    print(f\"Final {plot_metric}: {metric_values[-1]:.4f}\")\n",
    "    print(f\"Maximum {plot_metric}: {np.max(metric_values):.4f}\")\n",
    "\n",
    "\n",
    "fig.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613a3976",
   "metadata": {},
   "source": [
    "# Hyper parameter tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf39b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from report import open_experiment_report\n",
    "\n",
    "all_reports = open_experiment_report(\"../results/exp1_2c_mlp\")\n",
    "all_reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd141540",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ranking the runs\n",
    "metric = \"accuracy\"\n",
    "\n",
    "run_ranks = []\n",
    "for run, report in enumerate(all_reports):\n",
    "    final_metric = report.get_final(metric)\n",
    "    max_metric = report.get_max(metric)\n",
    "    run_ranks.append((run, final_metric, max_metric))\n",
    "\n",
    "run_ranks.sort(key=lambda x: x[1], reverse=True) \n",
    "print(f\"Ranks: {metric} \")\n",
    "for rank, (run, metric_value, max_value) in enumerate(run_ranks, start=1):\n",
    "    print(f\"{rank}: Run {run} with metric value {metric_value:.4f},  {max_value:.4f}  max\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615ab6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.set_title(all_reports[0].get_experiment_name())\n",
    "\n",
    "plot_metric = \"accuracy\"  # Define the metric to plot\n",
    "\n",
    "for run, FFoldResults in enumerate(all_reports):\n",
    "    print(f\"======= Results for Run {run}: ======\")\n",
    "    hyperparams = FFoldResults.hyperparameters()\n",
    "    print(f\"Hyperparameters: {hyperparams}\")\n",
    "    # il_config = FFoldResults.il_config()\n",
    "    # print(f\"IL Config: {il_config}\")\n",
    "    print(f\"{plot_metric}: {FFoldResults.get_max_final(plot_metric)}\")\n",
    "    \n",
    "    rounds, accuracy_mean, accuracy_std = FFoldResults.get_mean_std(plot_metric)\n",
    "    \n",
    "    ax.plot(rounds, accuracy_mean, '-o', label=f\"Run {run}\")\n",
    "    ax.fill_between(rounds, accuracy_mean - accuracy_std, accuracy_mean + accuracy_std, alpha=0.1)\n",
    "    \n",
    "    \n",
    "    rounds, loss_mean, loss_std = FFoldResults.get_mean_std(\"loss\")\n",
    "    ax.plot(rounds, loss_mean, '--', label=f\"Loss Run {run}\")\n",
    "\n",
    "fig.legend()\n",
    "\n",
    "ax.set_ylim(0.3,0.85)\n",
    "fig.set_size_inches(12, 8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe05db62",
   "metadata": {},
   "source": [
    "# Overfit graphs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933df03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d44efc2",
   "metadata": {},
   "source": [
    "# Select file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d7c498d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_content(path):\n",
    "    with open(path, \"r\") as file:\n",
    "        content = file.read()\n",
    "        json_content = json.loads(content)  \n",
    "        file.close()\n",
    "        return json_content\n",
    "\n",
    "# Select a results file\n",
    "\n",
    "def select_file(folder_path, idx):\n",
    "    results_files = os.listdir(folder_path)\n",
    "    for i, file in enumerate(results_files):\n",
    "        print(f\"{i}: {file}\")\n",
    "    if idx < 0 or idx >= len(results_files):\n",
    "        print(\"Invalid index. Please select a valid file index.\")\n",
    "        return None\n",
    "    selcted_file = results_files[idx]\n",
    "    print(\"Selected file: \", selcted_file)\n",
    "    \n",
    "    report = file_content(os.path.join(folder_path, selcted_file))\n",
    "    \n",
    "    return report\n",
    "\n",
    "# In case of hyperparameter tuning, the report file is named differently\n",
    "def report_file_name(run_id, fold_i):\n",
    "    return f\"report_{run_id}_fold_{fold_i}.json\"\n",
    "\n",
    "\n",
    "def open_single_report_file(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File {file_path} does not exist\")\n",
    "        return None\n",
    "    \n",
    "    report = file_content(file_path)\n",
    "    return report\n",
    "\n",
    "def open_5fcv_report(results_folder, run_id, fold_i):\n",
    "    file_name = report_file_name(run_id, fold_i)\n",
    "    file_path = os.path.join(results_folder, file_name)\n",
    "    return open_single_report_file(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f60ca9",
   "metadata": {},
   "source": [
    "# Report metrics extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0be73a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Report:\n",
    "    def __init__(self, report: dict):\n",
    "        self.report = report\n",
    "        self.clean()\n",
    "    \n",
    "    def clean(self):\n",
    "        try:\n",
    "            rounds = self.report[\"central_eval\"]\n",
    "            if not bool(rounds[0]): # skip empty rounds\n",
    "                self.report[\"central_eval\"] = rounds[1:]\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    \n",
    "    def hyperparams(self):\n",
    "        return self.report[\"hyperparams\"]\n",
    "\n",
    "    def extract_central_eval_metric(self, metric) -> tuple[np.ndarray, np.ndarray]:\n",
    "        rounds = self.report[\"central_eval\"]\n",
    "\n",
    "        metric_data = np.array([rounds[i][metric] for i in range(len(rounds))])\n",
    "        return metric_data\n",
    "\n",
    "\n",
    "    \n",
    "    def extract_fit_config_metric(self, metric) -> tuple[np.ndarray, np.ndarray]:\n",
    "        rounds = self.report[\"fit_config\"]\n",
    "        print(rounds)\n",
    "        rounds_data = np.array([rounds[i][\"round\"] for i in range(len(rounds))])\n",
    "        metric_data = np.array([rounds[i][metric] for i in range(len(rounds))])\n",
    "        return rounds_data, metric_data\n",
    "\n",
    "        \n",
    "class FiveFoldResults():\n",
    "    def __init__(self, reports: list[Report]):\n",
    "        self.reports = reports\n",
    "        self.validate_reports()\n",
    "        \n",
    "    def validate_reports(self):\n",
    "        if len(self.reports) != 5:\n",
    "            raise ValueError(\"There should be exactly 5 reports for 5-fold cross-validation.\")\n",
    "        \n",
    "        for i in range(1, len(self.reports)):\n",
    "            if self.reports[i].report[\"hyperparams\"] != self.reports[0].report[\"hyperparams\"]:\n",
    "                raise ValueError(\"Hyperparameters do not match across reports.\")   \n",
    "            \n",
    "            if self.reports[i].report[\"run_id\"] != self.reports[0].report[\"run_id\"]:\n",
    "                raise ValueError(\"Run IDs do not match across reports.\")\n",
    "            \n",
    "            \n",
    "    def get_experiment_name(self):\n",
    "        return self.reports[0].report[\"experiment_name\"]\n",
    "    \n",
    "    def get_start_time(self):\n",
    "        return self.reports[0].report[\"start_time\"]\n",
    "        \n",
    "    def hyperparameters(self):\n",
    "        return self.reports[0].report[\"hyperparams\"]\n",
    "        \n",
    "    def get_mean_std(self, metric: str) -> tuple[np.ndarray, np.ndarray]:\n",
    "        all_metrics = []\n",
    "        x = None\n",
    "        \n",
    "        for report in self.reports:\n",
    "            x, metric_values = report.extract_central_eval_metric(metric)\n",
    "            all_metrics.append(metric_values)\n",
    "            \n",
    "        all_metrics = np.array(all_metrics)\n",
    "        \n",
    "        mean = np.mean(all_metrics, axis=0)\n",
    "        std = np.std(all_metrics, axis=0)\n",
    "        \n",
    "        return x, mean, std\n",
    "    \n",
    "    def get_max_final(self, metric: str):\n",
    "        return self.get_max(metric), self.get_final(metric)\n",
    "    \n",
    "    def get_max(self, metric: str):\n",
    "        _, mean, _ = self.get_mean_std(metric)\n",
    "\n",
    "        return np.round(np.max(mean), 3)\n",
    "    \n",
    "    def get_final(self, metric: str):\n",
    "        _, mean, _ = self.get_mean_std(metric)\n",
    "        \n",
    "        return np.round(mean[-1], 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f349726a",
   "metadata": {},
   "source": [
    "# Single report resuls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9075b888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reports location and information\n",
    "report_path = \"./experiment3_domain_results/report_0_fold_0.json\"\n",
    "\n",
    "\n",
    "report_content = open_single_report_file(file_path=report_path)\n",
    "\n",
    "if report_content is None:\n",
    "    print(\"No report found\")\n",
    "    exit(1)\n",
    "\n",
    "report = Report(report_content)\n",
    "report.clean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb2bcff",
   "metadata": {},
   "source": [
    "# Multi report results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab97d488",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiReport:\n",
    "    def __init__(self, reports_folder: str, report_num: int):\n",
    "        self.reports = []\n",
    "        for i in range(5):\n",
    "            report = Report(open_single_report_file(file_path=f\"{reports_folder}report_{str(report_num)}_fold_{str(i)}.json\"))\n",
    "            self.reports.append(report)\n",
    "\n",
    "    def get_mean_var_metric(self, metric: str):\n",
    "        metrics = []\n",
    "        for report in self.reports:\n",
    "            try:\n",
    "                metric_list = report.extract_central_eval_metric(metric)\n",
    "                if len(metric_list) < 50:\n",
    "                    continue\n",
    "                metrics.append(metric_list)\n",
    "            except:\n",
    "                print(f\"Metric {metric} not found in report {self.hyperparams()}\")\n",
    "                pass\n",
    "        \n",
    "        metrics = np.array(metrics)\n",
    "\n",
    "        return np.mean(metrics, axis=0), np.std(metrics, axis=0)\n",
    "    \n",
    "    def hyperparams(self):\n",
    "        return self.reports[0].hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93e9951",
   "metadata": {},
   "outputs": [],
   "source": [
    "axis_font_size = 14\n",
    "title_font_size = 16\n",
    "fig_title_font_size = 18\n",
    "legend_font_size = 14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3692b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Plot:\n",
    "    def __init__(self, shape=(1,1)):\n",
    "        fig, ax = plt.subplots(shape[0], shape[1])\n",
    "        self.fig = fig\n",
    "        self.ax = ax\n",
    "        self.fig.set_size_inches(12, 6)\n",
    "        # Make space in the vertical axis\n",
    "        fig.subplots_adjust(hspace=0.5, wspace=0.3)\n",
    "\n",
    "    def plot_mean_std(self, index, mean_data: np.ndarray, std_data: np.ndarray, title: str = \"\", name: str = \"\", x_label: str = \"Rounds\", y_label: str = \"Metric\") -> tuple[plt.Figure, plt.Axes]:\n",
    "        rounds = np.arange(len(mean_data))\n",
    "\n",
    "        self.ax[index].plot(rounds, mean_data, label=f\"{title} (Mean)\")\n",
    "\n",
    "        self.ax[index].fill_between(\n",
    "            rounds,\n",
    "            mean_data - std_data,\n",
    "            mean_data + std_data,\n",
    "            alpha=0.1,\n",
    "            # label=f\"{name} (Standard Deviation)\",\n",
    "        )\n",
    "\n",
    "        self.ax[index].set_xlabel(x_label, fontsize=axis_font_size)\n",
    "        self.ax[index].set_ylabel(y_label, fontsize=axis_font_size)\n",
    "        self.ax[index].set_title(title, fontsize=title_font_size)\n",
    "\n",
    "        self.ax[index].set_ylim(0, 1)\n",
    "\n",
    "        self.ax[index].grid()\n",
    "        # self.ax[index].legend()\n",
    "        # Show last value of mean_data on the plot\n",
    "        # self.ax[index].text(\n",
    "        #     rounds[-1] + 5,\n",
    "        #     mean_data[-1],\n",
    "        #     f\"{mean_data[-1]:.3f}\",\n",
    "        #     fontsize=10,\n",
    "        #     verticalalignment=\"bottom\",\n",
    "        #     horizontalalignment=\"right\",\n",
    "        #     color=\"black\",\n",
    "        #     bbox=dict(facecolor=\"white\", edgecolor=\"black\", boxstyle=\"round,pad=0.3\"),\n",
    "        # )\n",
    "        \n",
    "\n",
    "        return self.fig, self.ax\n",
    "\n",
    "    def plot_all_metrics(self, index, metric: str, multireport: MultiReport, title: str = \"\", add_labels: bool = False):\n",
    "        def plot_mean_std_helper(mean_data: np.ndarray, std_data: np.ndarray, name: str = \"\"):\n",
    "            rounds = np.arange(len(mean_data))\n",
    "\n",
    "            self.ax[index].plot(rounds, mean_data, label=f\"{name}\") if add_labels else self.ax[index].plot(rounds, mean_data)\n",
    "\n",
    "            self.ax[index].fill_between(\n",
    "                rounds,\n",
    "                mean_data - std_data,\n",
    "                mean_data + std_data,\n",
    "                alpha=0.1,\n",
    "                # label=f\"{name} (Standard Deviation)\",\n",
    "            )\n",
    "\n",
    "        self.ax[index].set_ylim(0, 1)\n",
    "        for i in [\"01\", \"23\", \"45\", \"67\", \"89\"]:\n",
    "            mean, std = multireport.get_mean_var_metric(f\"{metric}_{i}\")\n",
    "            plot_mean_std_helper(mean, std, name=f\"Domain {i}\")\n",
    "        self.ax[index].set_title(f\"Accuracy on Client Domains\", fontsize=title_font_size)\n",
    "        self.ax[index].set_xlabel(\"Rounds\", fontsize=axis_font_size)\n",
    "        self.ax[index].set_ylabel(\"Accuracy\", fontsize=axis_font_size)\n",
    "        self.ax[index].set_title(title, fontsize=title_font_size)\n",
    "        self.ax[index].grid()\n",
    "\n",
    "        return self.fig, self.ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a62b8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr0 = MultiReport(\"./experiment3_domain_results/\", 0)\n",
    "mr1 = MultiReport(\"./experiment3_domain_results/\", 1)\n",
    "mr2 = MultiReport(\"./experiment3_domain_results/\", 2)\n",
    "mr3 = MultiReport(\"./experiment3_domain_results/\", 3)\n",
    "mr4 = MultiReport(\"./experiment3_EWC_results/\", 0)\n",
    "\n",
    "mean0, std0 = mr0.get_mean_var_metric(\"accuracy_global\")\n",
    "mean1, std1 = mr1.get_mean_var_metric(\"accuracy_global\")\n",
    "mean2, std2 = mr2.get_mean_var_metric(\"accuracy_global\")\n",
    "mean3, std3 = mr3.get_mean_var_metric(\"accuracy_global\")\n",
    "mean4, std4 = mr4.get_mean_var_metric(\"accuracy_global\")\n",
    "\n",
    "p = Plot(shape=(2, 3))\n",
    "\n",
    "p.plot_mean_std((0,0), mean0, std0, title=\"FedAvg\", name=\"Global Accuracy\", y_label=\"Accuracy\")\n",
    "p.plot_mean_std((0,1), mean1, std1, title=\"Fed-A-GEM\", name=\"Global Accuracy\", y_label=\"Accuracy\")\n",
    "p.plot_mean_std((1,0), mean2, std2, title=\"FedER 100%\", name=\"Global Accuracy\", y_label=\"Accuracy\")\n",
    "p.plot_mean_std((1,1), mean3, std3, title=\"FedER 0%\", name=\"Global Accuracy\", y_label=\"Accuracy\")\n",
    "fig, ax = p.plot_mean_std((0,2), mean4, std4, title=\"EWC lambda=1e5\", name=\"Global Accuracy\", y_label=\"Accuracy\")\n",
    "ax[1,2].set_visible(False)  # Hide the empty subplot\n",
    "\n",
    "# fig.suptitle(\"Global accuracy of every tested FL algorithm (FDIL)\", fontsize=fig_title_font_size)\n",
    "\n",
    "fig.savefig(\"exp3_FL_global_accuracy_domain.svg\", format=\"svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2a4955",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr = MultiReport(\"./experiment3_task_results/\", 3)\n",
    "mr = MultiReport(\"./experiment3_EWC_results/\", 1)\n",
    "\n",
    "mean_global, std_global = mr.get_mean_var_metric(\"accuracy_global\")\n",
    "mean_01, std_01 = mr.get_mean_var_metric(\"accuracy_01\")\n",
    "mean_23, std_23 = mr.get_mean_var_metric(\"accuracy_23\")\n",
    "mean_45, std_45 = mr.get_mean_var_metric(\"accuracy_45\")\n",
    "mean_67, std_67 = mr.get_mean_var_metric(\"accuracy_67\")\n",
    "mean_89, std_89 = mr.get_mean_var_metric(\"accuracy_89\")\n",
    "\n",
    "def print_final_mean_std(mean, std, label, bold=False):\n",
    "    \"\"\"Print the final mean and std of the accuracies.\"\"\"\n",
    "    if bold:\n",
    "        print(\"\\\\acctstdbold{\" + f\"{mean:.3f}\" +\"}{\" + f\"{std:.3f}\" +\"}\")\n",
    "    else:\n",
    "        print(\"\\\\acctstd{\" + f\"{mean:.3f}\" +\"}{\" + f\"{std:.3f}\" +\"}\")\n",
    "\n",
    "print_final_mean_std(mean_global[-1], std_global[-1], \"Global\", bold=True)\n",
    "print_final_mean_std(mean_01[-1], std_01[-1], \"0's and 1's\")\n",
    "print_final_mean_std(mean_23[-1], std_23[-1], \"2's and 3's\")\n",
    "print_final_mean_std(mean_45[-1], std_45[-1], \"4's and 5's\")\n",
    "print_final_mean_std(mean_67[-1], std_67[-1], \"6's and 7's\")\n",
    "print_final_mean_std(mean_89[-1], std_89[-1], \"8's and 9's\")\n",
    "\n",
    "mr.hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba50f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr0 = MultiReport(\"./experiment3_domain_results/\", 0)\n",
    "mr1 = MultiReport(\"./experiment3_domain_results/\", 1)\n",
    "mr2 = MultiReport(\"./experiment3_domain_results/\", 2)\n",
    "mr3 = MultiReport(\"./experiment3_domain_results/\", 3)\n",
    "mr4 = MultiReport(\"./experiment3_EWC_results/\", 0)\n",
    "\n",
    "\n",
    "p = Plot(shape=(2, 3))\n",
    "\n",
    "p.plot_all_metrics((0, 0), \"accuracy\", mr0, title=\"Standard FL\", add_labels=True)\n",
    "p.plot_all_metrics((0, 1), \"accuracy\", mr1, title=\"Fed-A-GEM\")\n",
    "p.plot_all_metrics((1, 0), \"accuracy\", mr2, title=\"FedER 100%\")\n",
    "p.plot_all_metrics((1, 1), \"accuracy\", mr3, title=\"FedER 0%\")\n",
    "fig, ax = p.plot_all_metrics((0, 2), \"accuracy\", mr4, title=\"EWC lambda=1e5\")\n",
    "ax[1,2].set_visible(False)  # Hide the empty subplot\n",
    "\n",
    "# fig.suptitle(\"Client accuracies of every tested FL algorithm (FDIL)\", fontsize=fig_title_font_size)\n",
    "fig.legend(loc='lower center', ncol=5, fontsize=legend_font_size)\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "\n",
    "fig.savefig(\"exp3_FL_domain_accuracy_domain.svg\", format=\"svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac57bbe9",
   "metadata": {},
   "source": [
    "Ved hurtigt kig igennem de kilder vi har brugt, så er det her novel for normal MNIST dataset.\n",
    "\n",
    "Dog stemmer resultaterne, om at Fed-A-GEM forbedrer resultaterne, overens med dem i artiklen: https://arxiv.org/pdf/2409.01585"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a48657e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "mr0 = MultiReport(\"./experiment3_task_results/\", 0)\n",
    "mr1 = MultiReport(\"./experiment3_task_results/\", 1)\n",
    "mr2 = MultiReport(\"./experiment3_task_results/\", 2)\n",
    "mr3 = MultiReport(\"./experiment3_task_results/\", 3)\n",
    "mr4 = MultiReport(\"./experiment3_EWC_results/\", 1)\n",
    "\n",
    "\n",
    "mean0, std0 = mr0.get_mean_var_metric(\"accuracy_global\")\n",
    "mean1, std1 = mr1.get_mean_var_metric(\"accuracy_global\")\n",
    "mean2, std2 = mr2.get_mean_var_metric(\"accuracy_global\")\n",
    "mean3, std3 = mr3.get_mean_var_metric(\"accuracy_global\")\n",
    "mean4, std4 = mr4.get_mean_var_metric(\"accuracy_global\")\n",
    "\n",
    "p = Plot(shape=(2,3 ))\n",
    "\n",
    "p.plot_mean_std((0,0), mean0, std0, title=\"FedAvg\", name=\"Global Accuracy\", y_label=\"Accuracy\")\n",
    "p.plot_mean_std((0,1), mean1, std1, title=\"Fed-A-GEM\", name=\"Global Accuracy\", y_label=\"Accuracy\")\n",
    "p.plot_mean_std((1,0), mean2, std2, title=\"FedER 100%\", name=\"Global Accuracy\", y_label=\"Accuracy\")\n",
    "p.plot_mean_std((1,1), mean3, std3, title=\"FedER 0%\", name=\"Global Accuracy\", y_label=\"Accuracy\")\n",
    "fig, ax = p.plot_mean_std((0,2), mean4, std4, title=\"EWC lambda=1e2\", name=\"Global Accuracy\", y_label=\"Accuracy\")\n",
    "ax[1,2].set_visible(False)  # Hide the empty subplot\n",
    "\n",
    "# fig.suptitle(\"Global accuracy of every tested FL algorithm (FTIL)\", fontsize=fig_title_font_size)\n",
    "\n",
    "fig.savefig(\"exp3_FL_global_accuracy_task.svg\", format=\"svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f124693a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mr0 = MultiReport(\"./experiment3_task_results/\", 0)\n",
    "mr1 = MultiReport(\"./experiment3_task_results/\", 1)\n",
    "mr2 = MultiReport(\"./experiment3_task_results/\", 2)\n",
    "mr3 = MultiReport(\"./experiment3_task_results/\", 3)\n",
    "mr4 = MultiReport(\"./experiment3_EWC_results/\", 1)\n",
    "\n",
    "\n",
    "p = Plot(shape=(2, 3))\n",
    "\n",
    "p.plot_all_metrics((0, 0), \"accuracy\", mr0, title=\"Standard FL\", add_labels=True)\n",
    "p.plot_all_metrics((0, 1), \"accuracy\", mr1, title=\"Fed-A-GEM\")\n",
    "p.plot_all_metrics((1, 0), \"accuracy\", mr2, title=\"FedER 100%\")\n",
    "p.plot_all_metrics((1, 1), \"accuracy\", mr3, title=\"FedER 0%\")\n",
    "fig, ax = p.plot_all_metrics((0, 2), \"accuracy\", mr4, title=\"EWC lambda=1e2\")\n",
    "ax[1,2].set_visible(False)  # Hide the empty subplot\n",
    "\n",
    "# fig.suptitle(\"Client accuracies of every tested FL algorithm (FTIL)\", fontsize=fig_title_font_size)\n",
    "fig.legend(loc='lower center', ncol=5, fontsize=legend_font_size)\n",
    "fig.subplots_adjust(bottom=0.2)\n",
    "\n",
    "fig.savefig(\"exp3_FL_domain_accuracy_task.svg\", format=\"svg\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e25e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrtest1 = MultiReport(\"./experiment3_task_20roundsPT_results/\", 0)\n",
    "mrtest2 = MultiReport(\"./experiment3_task_20roundsPT_results/\", 1)\n",
    "\n",
    "p = Plot(shape=(2, 2))\n",
    "\n",
    "p.plot_all_metrics((0, 0), \"Domain \", mrtest1, title=\"Fed-A-GEM 20Rounds/task\", add_labels=True)\n",
    "p.plot_all_metrics((1, 0), \"accuracy\", mrtest2, title=\"Fed-ER 20Rounds/task\", add_labels=True)\n",
    "meantest1, stdtest1 = mrtest1.get_mean_var_metric(\"accuracy_global\")\n",
    "p.plot_mean_std((0,1), meantest1, stdtest1, title=\"Fed-A-GEM 20Rounds/task\", name=\"Global Accuracy\", y_label=\"Accuracy\")\n",
    "meantest2, stdtest2 = mrtest2.get_mean_var_metric(\"accuracy_global\")\n",
    "p.plot_mean_std((1,1), meantest2, stdtest2, title=\"Fed-ER 20Rounds/task\", name=\"Global Accuracy\", y_label=\"Accuracy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed5b0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "315cc968",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176392e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr_datasets.partitioner import IidPartitioner\n",
    "from torch.utils.data import DataLoader\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "\n",
    "class ClinicPartitioner(IidPartitioner):\n",
    "    \"\"\"Partitioner for splitting MNIST into 5 centers.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(num_partitions=5)\n",
    "\n",
    "    def load_partition(self, partition_id: int) -> datasets.Dataset:\n",
    "        \"\"\"\n",
    "        Creates 5 partitions of the dataset:\n",
    "            1. 0's and 1's\n",
    "            2. 2's and 3's\n",
    "            3. 4's and 5's\n",
    "            4. 6's and 7's\n",
    "            5. 8's and 9's\n",
    "        \"\"\"\n",
    "\n",
    "        # Return the entire dataset if partition_id is -1\n",
    "        if (partition_id == -1):\n",
    "            return self.dataset\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"image\": [np.array(img) for img in self.dataset[\"image\"]],\n",
    "                \"label\": self.dataset[\"label\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        df1 = df[df[\"label\"].isin([0, 1])]\n",
    "        df2 = df[df[\"label\"].isin([2, 3])]\n",
    "        df3 = df[df[\"label\"].isin([4, 5])]\n",
    "        df4 = df[df[\"label\"].isin([6, 7])]\n",
    "        df5 = df[df[\"label\"].isin([8, 9])]\n",
    "\n",
    "        def convertPDtoDS(df):\n",
    "            return datasets.Dataset.from_dict({\n",
    "                \"image\": [Image.fromarray(np.array(img)) for img in df[\"image\"]],\n",
    "                \"label\": df[\"label\"],\n",
    "            })\n",
    "\n",
    "        splitDataset = [\n",
    "            convertPDtoDS(df1),\n",
    "            convertPDtoDS(df2),\n",
    "            convertPDtoDS(df3),\n",
    "            convertPDtoDS(df4),\n",
    "            convertPDtoDS(df5),\n",
    "        ]\n",
    "\n",
    "        return splitDataset[partition_id]\n",
    "\n",
    "\n",
    "trainPartitioner = None\n",
    "testPartitioner = None\n",
    "\n",
    "\n",
    "def load_data(partition_id: int, split: int):\n",
    "    \"\"\"Load partition MNIST data.\"\"\"\n",
    "    # Only initialize `FederatedDataset` once\n",
    "    global trainPartitioner, testPartitioner\n",
    "    if trainPartitioner is None or testPartitioner is None:\n",
    "        trainPartitioner = ClinicPartitioner()\n",
    "        testPartitioner = ClinicPartitioner()\n",
    "        ds = datasets.load_dataset(path=\"ylecun/mnist\")\n",
    "        ds = datasets.concatenate_datasets([ds[\"train\"], ds[\"test\"]])\n",
    "        trainPartitioner.dataset = datasets.concatenate_datasets([ds.shard(5, i) for i in list(filter(lambda x: x != split, range(5)))])\n",
    "        testPartitioner.dataset = ds.shard(5, split)\n",
    "    partition_train = trainPartitioner.load_partition(partition_id)\n",
    "    partition_test = testPartitioner.load_partition(partition_id)\n",
    "\n",
    "    def apply_transforms(batch):\n",
    "        \"\"\"Apply transforms to the partition from FederatedDataset.\"\"\"\n",
    "        batch[\"image\"] = [(np.array(img, dtype=np.float32) / 256).flatten() for img in batch[\"image\"]] # Transform images to float and normalize\n",
    "        return batch\n",
    "\n",
    "\n",
    "    partition_train = partition_train.with_transform(apply_transforms)\n",
    "    partition_test = partition_test.with_transform(apply_transforms)\n",
    "    trainloader = DataLoader(partition_train, batch_size=32, shuffle=True)\n",
    "    testloader = DataLoader(partition_test, batch_size=32)\n",
    "    return trainloader, testloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422a29b8",
   "metadata": {},
   "source": [
    "# Get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81108535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class NetDomainIL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NetDomainIL, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(28*28, 40)\n",
    "        self.fc2 = nn.Linear(40, 20)\n",
    "        self.fc3 = nn.Linear(20, 1)\n",
    "\n",
    "    def forward(self, x, i):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        next_last_layer_1st_neuron = self.fc2(x)[:, i]\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return torch.sigmoid(self.fc3(x)), next_last_layer_1st_neuron\n",
    "\n",
    "\n",
    "\n",
    "def trainDomainIL(net: nn.Module, trainloader, device):\n",
    "    net.to(device)  # move model to GPU if available\n",
    "    net.train()\n",
    "    \n",
    "    # Create optimizer\n",
    "    criterion = torch.nn.BCELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "    optimizer.zero_grad()\n",
    "    cumulated_loss = 0.0\n",
    "    \n",
    "    # Update gradient in the model\n",
    "    for batch in trainloader:\n",
    "        images = batch[\"image\"].to(device)\n",
    "        labels = (batch[\"label\"] % 2).to(device)\n",
    "        labels = labels.cpu().type(torch.float32) [:, torch.newaxis]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs, _ = net(images, 0)\n",
    "\n",
    "        # Get the loss and compute the gradient\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Apply gradient\n",
    "        optimizer.step()\n",
    "\n",
    "        cumulated_loss += loss.item()\n",
    "    \n",
    "    return cumulated_loss / len(trainloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1db86f",
   "metadata": {},
   "source": [
    "# Train and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a40808",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NetDomainIL()\n",
    "i = 0\n",
    "trainloader_global, testloader_global = load_data(-1, i)\n",
    "_, testloader01 = load_data(0, i)\n",
    "_, testloader23 = load_data(1, i)\n",
    "_, testloader45 = load_data(2, i)\n",
    "_, testloader67 = load_data(3, i)\n",
    "_, testloader89 = load_data(4, i)\n",
    "\n",
    "# Get initial performance\n",
    "epocs = 50\n",
    "for i in range(epocs):\n",
    "    print(f\"Epoch {i+1}/{epocs}\")\n",
    "    # Train the model\n",
    "    train_loss = trainDomainIL(net, trainloader_global, \"cpu\")\n",
    "    print(f\"Epoch {i+1}/{epocs}, train_loss: {train_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a26183",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinicPartitioner2(IidPartitioner):\n",
    "    \"\"\"Partitioner for splitting MNIST into 5 centers.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(num_partitions=10)\n",
    "\n",
    "    def load_partition(self, partition_id: int) -> datasets.Dataset:\n",
    "        \"\"\"\n",
    "        Creates 5 partitions of the dataset:\n",
    "            1. 0's and 1's\n",
    "            2. 2's and 3's\n",
    "            3. 4's and 5's\n",
    "            4. 6's and 7's\n",
    "            5. 8's and 9's\n",
    "        \"\"\"\n",
    "\n",
    "        # Return the entire dataset if partition_id is -1\n",
    "        if (partition_id == -1):\n",
    "            return self.dataset\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"image\": [np.array(img) for img in self.dataset[\"image\"]],\n",
    "                \"label\": self.dataset[\"label\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        df0 = df[df[\"label\"].isin([0])]\n",
    "        df1 = df[df[\"label\"].isin([1])]\n",
    "        df2 = df[df[\"label\"].isin([2])]\n",
    "        df3 = df[df[\"label\"].isin([3])]\n",
    "        df4 = df[df[\"label\"].isin([4])]\n",
    "        df5 = df[df[\"label\"].isin([5])]\n",
    "        df6 = df[df[\"label\"].isin([6])]\n",
    "        df7 = df[df[\"label\"].isin([7])]\n",
    "        df8 = df[df[\"label\"].isin([8])]\n",
    "        df9 = df[df[\"label\"].isin([9])]\n",
    "\n",
    "        def convertPDtoDS(df):\n",
    "            return datasets.Dataset.from_dict({\n",
    "                \"image\": [Image.fromarray(np.array(img)) for img in df[\"image\"]],\n",
    "                \"label\": df[\"label\"],\n",
    "            })\n",
    "\n",
    "        splitDataset = [\n",
    "            convertPDtoDS(df0),\n",
    "            convertPDtoDS(df1),\n",
    "            convertPDtoDS(df2),\n",
    "            convertPDtoDS(df3),\n",
    "            convertPDtoDS(df4),\n",
    "            convertPDtoDS(df5),\n",
    "            convertPDtoDS(df6),\n",
    "            convertPDtoDS(df7),\n",
    "            convertPDtoDS(df8),\n",
    "            convertPDtoDS(df9),\n",
    "        ]\n",
    "\n",
    "        return splitDataset[partition_id]\n",
    "\n",
    "trainPartitioner2 = None\n",
    "testPartitioner2 = None\n",
    "\n",
    "def load_data_2(partition_id: int, split: int):\n",
    "    \"\"\"Load partition MNIST data.\"\"\"\n",
    "    # Only initialize `FederatedDataset` once\n",
    "    global trainPartitioner2, testPartitioner2\n",
    "    if trainPartitioner2 is None or testPartitioner2 is None:\n",
    "        trainPartitioner2 = ClinicPartitioner2()\n",
    "        testPartitioner2 = ClinicPartitioner2()\n",
    "        ds = datasets.load_dataset(path=\"ylecun/mnist\")\n",
    "        ds = datasets.concatenate_datasets([ds[\"train\"], ds[\"test\"]])\n",
    "        trainPartitioner2.dataset = datasets.concatenate_datasets([ds.shard(5, i) for i in list(filter(lambda x: x != split, range(5)))])\n",
    "        testPartitioner2.dataset = ds.shard(5, split)\n",
    "    partition_train = trainPartitioner2.load_partition(partition_id)\n",
    "    partition_test = testPartitioner2.load_partition(partition_id)\n",
    "\n",
    "    def apply_transforms(batch):\n",
    "        \"\"\"Apply transforms to the partition from FederatedDataset.\"\"\"\n",
    "        batch[\"image\"] = [(np.array(img, dtype=np.float32) / 256).flatten() for img in batch[\"image\"]] # Transform images to float and normalize\n",
    "        return batch\n",
    "\n",
    "\n",
    "    partition_train = partition_train.with_transform(apply_transforms)\n",
    "    partition_test = partition_test.with_transform(apply_transforms)\n",
    "    trainloader = DataLoader(partition_train, batch_size=32, shuffle=True)\n",
    "    testloader = DataLoader(partition_test, batch_size=32)\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a42bfc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testDomainIL(net, testloader, device, index=0):\n",
    "    \"\"\"Validate the model on the test set.\"\"\"\n",
    "    net.to(device)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    loss = 0.0\n",
    "    all_nll1stoutput = np.array([]) \n",
    "    with torch.no_grad():\n",
    "        for batch in testloader:\n",
    "            images = batch[\"image\"].to(device)\n",
    "            labels = (batch[\"label\"] % 2).to(device) # Convert to binary labels (0 or 1)\n",
    "            labels = labels.cpu().type(torch.float32) [:, torch.newaxis]\n",
    "            outputs, nll1stoutput = net(images, index)\n",
    "            loss += criterion(outputs, labels).item()\n",
    "            all_nll1stoutput = np.concat([all_nll1stoutput, nll1stoutput.cpu().numpy()])\n",
    "    loss = loss / len(testloader)\n",
    "    return loss, all_nll1stoutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d068db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, loader0 = load_data_2(0, 0)\n",
    "_, loader1 = load_data_2(1, 0)\n",
    "_, loader2 = load_data_2(2, 0)\n",
    "_, loader3 = load_data_2(3, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0f7c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, nll1stoutput0 = testDomainIL(net, loader0, \"cpu\", index=1)\n",
    "_, nll1stoutput1 = testDomainIL(net, loader1, \"cpu\", index=1)\n",
    "_, nll1stoutput2 = testDomainIL(net, loader2, \"cpu\", index=1)\n",
    "_, nll1stoutput3 = testDomainIL(net, loader3, \"cpu\", index=1)\n",
    "\n",
    "# Make histogram of nll1stoutput\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "\n",
    "ax[0].hist(nll1stoutput0, bins=50, alpha=0.5, label='0s', color='blue')\n",
    "ax[0].hist(nll1stoutput1, bins=50, alpha=0.5, label='1s', color='orange')\n",
    "ax[1].hist(nll1stoutput2, bins=50, alpha=0.5, label='2s', color='green')\n",
    "ax[1].hist(nll1stoutput3, bins=50, alpha=0.5, label='3s', color='red')\n",
    "# Set common xlabel and ylabel for subplots\n",
    "ax[1].set_xlabel('Activation of 1st neuron in 2\\'nd last layer')\n",
    "ax[0].set_ylabel('Occurrences')\n",
    "ax[1].set_ylabel('Occurrences')\n",
    "\n",
    "ax[0].set_xlim(-2, 5)\n",
    "ax[1].set_xlim(-2, 5)\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "\n",
    "fig.savefig(\"MNISTDomainShiftClassifier.svg\", format='svg', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75794504",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

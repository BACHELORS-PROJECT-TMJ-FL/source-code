{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "315cc968",
   "metadata": {},
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176392e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flwr_datasets.partitioner import IidPartitioner\n",
    "from torch.utils.data import DataLoader\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "\n",
    "class ClinicPartitioner(IidPartitioner):\n",
    "    \"\"\"Partitioner for splitting MNIST into 5 centers.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(num_partitions=5)\n",
    "\n",
    "    def load_partition(self, partition_id: int) -> datasets.Dataset:\n",
    "        \"\"\"\n",
    "        Creates 5 partitions of the dataset:\n",
    "            1. 0's and 1's\n",
    "            2. 2's and 3's\n",
    "            3. 4's and 5's\n",
    "            4. 6's and 7's\n",
    "            5. 8's and 9's\n",
    "        \"\"\"\n",
    "\n",
    "        # Return the entire dataset if partition_id is -1\n",
    "        if (partition_id == -1):\n",
    "            return self.dataset\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"image\": [np.array(img) for img in self.dataset[\"image\"]],\n",
    "                \"label\": self.dataset[\"label\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        df1 = df[df[\"label\"].isin([0, 1])]\n",
    "        df2 = df[df[\"label\"].isin([2, 3])]\n",
    "        df3 = df[df[\"label\"].isin([4, 5])]\n",
    "        df4 = df[df[\"label\"].isin([6, 7])]\n",
    "        df5 = df[df[\"label\"].isin([8, 9])]\n",
    "\n",
    "        def convertPDtoDS(df):\n",
    "            return datasets.Dataset.from_dict({\n",
    "                \"image\": [Image.fromarray(np.array(img)) for img in df[\"image\"]],\n",
    "                \"label\": df[\"label\"],\n",
    "            })\n",
    "\n",
    "        splitDataset = [\n",
    "            convertPDtoDS(df1),\n",
    "            convertPDtoDS(df2),\n",
    "            convertPDtoDS(df3),\n",
    "            convertPDtoDS(df4),\n",
    "            convertPDtoDS(df5),\n",
    "        ]\n",
    "\n",
    "        return splitDataset[partition_id]\n",
    "\n",
    "\n",
    "trainPartitioner = None\n",
    "testPartitioner = None\n",
    "\n",
    "\n",
    "def load_data(partition_id: int, split: int):\n",
    "    \"\"\"Load partition MNIST data.\"\"\"\n",
    "    # Only initialize `FederatedDataset` once\n",
    "    global trainPartitioner, testPartitioner\n",
    "    if trainPartitioner is None or testPartitioner is None:\n",
    "        trainPartitioner = ClinicPartitioner()\n",
    "        testPartitioner = ClinicPartitioner()\n",
    "        ds = datasets.load_dataset(path=\"ylecun/mnist\")\n",
    "        ds = datasets.concatenate_datasets([ds[\"train\"], ds[\"test\"]])\n",
    "        trainPartitioner.dataset = datasets.concatenate_datasets([ds.shard(5, i) for i in list(filter(lambda x: x != split, range(5)))])\n",
    "        testPartitioner.dataset = ds.shard(5, split)\n",
    "    partition_train = trainPartitioner.load_partition(partition_id)\n",
    "    partition_test = testPartitioner.load_partition(partition_id)\n",
    "\n",
    "    def apply_transforms(batch):\n",
    "        \"\"\"Apply transforms to the partition from FederatedDataset.\"\"\"\n",
    "        batch[\"image\"] = [(np.array(img, dtype=np.float32) / 256).flatten() for img in batch[\"image\"]] # Transform images to float and normalize\n",
    "        return batch\n",
    "\n",
    "\n",
    "    partition_train = partition_train.with_transform(apply_transforms)\n",
    "    partition_test = partition_test.with_transform(apply_transforms)\n",
    "    trainloader = DataLoader(partition_train, batch_size=32, shuffle=True)\n",
    "    testloader = DataLoader(partition_test, batch_size=32)\n",
    "    return trainloader, testloader\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422a29b8",
   "metadata": {},
   "source": [
    "# Get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81108535",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "# Define autoencoder model\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(784, 32),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(32, 784),\n",
    "            nn.ReLU(),\n",
    "            nn.Sigmoid(),  # Use Sigmoid to ensure output is in [0, 1]\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "    \n",
    "    def get_embedding(self, x):\n",
    "        \"\"\"Get the embedding from the encoder.\"\"\"\n",
    "        return self.encoder(x)\n",
    "    \n",
    "# Define train function\n",
    "def train(model, trainloader, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.MSELoss()\n",
    "    for data in trainloader:\n",
    "        inputs = data[\"image\"].to(device)\n",
    "        inputs = inputs.view(inputs.size(0), -1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, inputs)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(trainloader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1db86f",
   "metadata": {},
   "source": [
    "# Train and test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a40808",
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Autoencoder()\n",
    "i = 0\n",
    "trainloader_global, testloader_global = load_data(-1, i)\n",
    "# _, testloader01 = load_data(0, i)\n",
    "# _, testloader23 = load_data(1, i)\n",
    "# _, testloader45 = load_data(2, i)\n",
    "# _, testloader67 = load_data(3, i)\n",
    "# _, testloader89 = load_data(4, i)\n",
    "\n",
    "# Get initial performance\n",
    "epocs = 20\n",
    "for i in range(epocs):\n",
    "    print(f\"Epoch {i+1}/{epocs}\")\n",
    "    # Train the model\n",
    "    train_loss = train(net, trainloader_global, \"cpu\")\n",
    "    print(f\"Epoch {i+1}/{epocs}, train_loss: {train_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a26183",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClinicPartitioner2(IidPartitioner):\n",
    "    \"\"\"Partitioner for splitting MNIST into 5 centers.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__(num_partitions=10)\n",
    "\n",
    "    def load_partition(self, partition_id: int) -> datasets.Dataset:\n",
    "        \"\"\"\n",
    "        Creates 5 partitions of the dataset:\n",
    "            1. 0's and 1's\n",
    "            2. 2's and 3's\n",
    "            3. 4's and 5's\n",
    "            4. 6's and 7's\n",
    "            5. 8's and 9's\n",
    "        \"\"\"\n",
    "\n",
    "        # Return the entire dataset if partition_id is -1\n",
    "        if (partition_id == -1):\n",
    "            return self.dataset\n",
    "\n",
    "        df = pd.DataFrame(\n",
    "            {\n",
    "                \"image\": [np.array(img) for img in self.dataset[\"image\"]],\n",
    "                \"label\": self.dataset[\"label\"],\n",
    "            }\n",
    "        )\n",
    "\n",
    "        df0 = df[df[\"label\"].isin([0])]\n",
    "        df1 = df[df[\"label\"].isin([1])]\n",
    "        df2 = df[df[\"label\"].isin([2])]\n",
    "        df3 = df[df[\"label\"].isin([3])]\n",
    "        df4 = df[df[\"label\"].isin([4])]\n",
    "        df5 = df[df[\"label\"].isin([5])]\n",
    "        df6 = df[df[\"label\"].isin([6])]\n",
    "        df7 = df[df[\"label\"].isin([7])]\n",
    "        df8 = df[df[\"label\"].isin([8])]\n",
    "        df9 = df[df[\"label\"].isin([9])]\n",
    "\n",
    "        def convertPDtoDS(df):\n",
    "            return datasets.Dataset.from_dict({\n",
    "                \"image\": [Image.fromarray(np.array(img)) for img in df[\"image\"]],\n",
    "                \"label\": df[\"label\"],\n",
    "            })\n",
    "\n",
    "        splitDataset = [\n",
    "            convertPDtoDS(df0),\n",
    "            convertPDtoDS(df1),\n",
    "            convertPDtoDS(df2),\n",
    "            convertPDtoDS(df3),\n",
    "            convertPDtoDS(df4),\n",
    "            convertPDtoDS(df5),\n",
    "            convertPDtoDS(df6),\n",
    "            convertPDtoDS(df7),\n",
    "            convertPDtoDS(df8),\n",
    "            convertPDtoDS(df9),\n",
    "        ]\n",
    "\n",
    "        return splitDataset[partition_id]\n",
    "\n",
    "trainPartitioner2 = None\n",
    "testPartitioner2 = None\n",
    "\n",
    "def load_data_2(partition_id: int, split: int):\n",
    "    \"\"\"Load partition MNIST data.\"\"\"\n",
    "    # Only initialize `FederatedDataset` once\n",
    "    global trainPartitioner2, testPartitioner2\n",
    "    if trainPartitioner2 is None or testPartitioner2 is None:\n",
    "        trainPartitioner2 = ClinicPartitioner2()\n",
    "        testPartitioner2 = ClinicPartitioner2()\n",
    "        ds = datasets.load_dataset(path=\"ylecun/mnist\")\n",
    "        ds = datasets.concatenate_datasets([ds[\"train\"], ds[\"test\"]])\n",
    "        trainPartitioner2.dataset = datasets.concatenate_datasets([ds.shard(5, i) for i in list(filter(lambda x: x != split, range(5)))])\n",
    "        testPartitioner2.dataset = ds.shard(5, split)\n",
    "    partition_train = trainPartitioner2.load_partition(partition_id)\n",
    "    partition_test = testPartitioner2.load_partition(partition_id)\n",
    "\n",
    "    def apply_transforms(batch):\n",
    "        \"\"\"Apply transforms to the partition from FederatedDataset.\"\"\"\n",
    "        batch[\"image\"] = [(np.array(img, dtype=np.float32) / 256).flatten() for img in batch[\"image\"]] # Transform images to float and normalize\n",
    "        return batch\n",
    "\n",
    "\n",
    "    partition_train = partition_train.with_transform(apply_transforms)\n",
    "    partition_test = partition_test.with_transform(apply_transforms)\n",
    "    trainloader = DataLoader(partition_train, batch_size=32, shuffle=True)\n",
    "    testloader = DataLoader(partition_test, batch_size=32)\n",
    "    return trainloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bcbfee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embeddings(net, loader):\n",
    "    \"\"\"Get embeddings for the dataset.\"\"\"\n",
    "    net.eval()\n",
    "    embeddings = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            inputs = data[\"image\"].to(\"cpu\")\n",
    "            inputs = inputs.view(inputs.size(0), -1)\n",
    "            embedding = net.get_embedding(inputs)\n",
    "            embeddings.append(embedding.cpu().numpy())\n",
    "    \n",
    "    embeddings = np.concatenate(embeddings, axis=0)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d068db55",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, loader0 = load_data_2(0, 0)\n",
    "_, loader1 = load_data_2(1, 0)\n",
    "_, loader2 = load_data_2(2, 0)\n",
    "_, loader3 = load_data_2(3, 0)\n",
    "\n",
    "print(\"Loaded data\")\n",
    "\n",
    "nll1stoutput0 = get_embeddings(net, loader0)\n",
    "nll1stoutput1 = get_embeddings(net, loader1)\n",
    "nll1stoutput2 = get_embeddings(net, loader2)\n",
    "nll1stoutput3 = get_embeddings(net, loader3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63fc0aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make histogram of nll1stoutput\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "\n",
    "ax[0].hist(nll1stoutput0[:, 5], bins=50, alpha=0.5, label='0s', color='blue')\n",
    "ax[0].hist(nll1stoutput1[:, 5], bins=50, alpha=0.5, label='1s', color='orange')\n",
    "ax[1].hist(nll1stoutput2[:, 5], bins=50, alpha=0.5, label='2s', color='green')\n",
    "ax[1].hist(nll1stoutput3[:, 5], bins=50, alpha=0.5, label='3s', color='red')\n",
    "# Set common xlabel and ylabel for subplots\n",
    "ax[1].set_xlabel('Single feature of reduced representation')\n",
    "ax[0].set_ylabel('Occurrences')\n",
    "ax[1].set_ylabel('Occurrences')\n",
    "\n",
    "ax[0].set_xlim(-10, 10)\n",
    "ax[1].set_xlim(-10, 10)\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed9491e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make histogram of nll1stoutput\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "\n",
    "ax[0].hist(nll1stoutput0[:, 7], bins=50, alpha=0.5, label='0s', color='blue')\n",
    "ax[0].hist(nll1stoutput1[:, 7], bins=50, alpha=0.5, label='1s', color='orange')\n",
    "ax[1].hist(nll1stoutput2[:, 7], bins=50, alpha=0.5, label='2s', color='green')\n",
    "ax[1].hist(nll1stoutput3[:, 7], bins=50, alpha=0.5, label='3s', color='red')\n",
    "# Set common xlabel and ylabel for subplots\n",
    "ax[1].set_xlabel('Single feature of reduced representation')\n",
    "ax[0].set_ylabel('Occurrences')\n",
    "ax[1].set_ylabel('Occurrences')\n",
    "\n",
    "ax[0].set_xlim(-10, 10)\n",
    "ax[1].set_xlim(-10, 10)\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f58b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make histogram of nll1stoutput\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "\n",
    "ax[0].hist(nll1stoutput0[:, 0], bins=50, alpha=0.5, label='0s', color='blue')\n",
    "ax[0].hist(nll1stoutput1[:, 0], bins=50, alpha=0.5, label='1s', color='orange')\n",
    "ax[1].hist(nll1stoutput2[:, 0], bins=50, alpha=0.5, label='2s', color='green')\n",
    "ax[1].hist(nll1stoutput3[:, 0], bins=50, alpha=0.5, label='3s', color='red')\n",
    "# Set common xlabel and ylabel for subplots\n",
    "ax[1].set_xlabel('Single feature of reduced representation')\n",
    "ax[0].set_ylabel('Occurrences')\n",
    "ax[1].set_ylabel('Occurrences')\n",
    "\n",
    "ax[0].set_xlim(-10, 10)\n",
    "ax[1].set_xlim(-10, 10)\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "\n",
    "fig.savefig(\"MNISTDomainShiftAutoEncoder.svg\", format='svg', bbox_inches='tight', dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d5f052",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
